<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
  <link rel="stylesheet" href="psg.css" type="text/css">
  <LINK REL="SHORTCUT ICON" HREF="favicon.ico" type="image/x-icon"/>
  <META NAME="description" content="System Administrator Pocket Survival Guide -  A series of notes for Sys Admin"/>
  <META NAME="keyword" content="Sys Admin, System Administrator, Solaris, HP-UX, AIX, Linux, Note, Notes, Pocket, Survival, Guide, psg, data center, power, electrical, plug, LYS, LKS, LAPPLAPP, PSG101, sn50, tin6150"/>
  <META NAME="Robots" CONTENT="all"/>
  <META NAME="Author" CONTENT="Tin Ho"/>

  <title>Pocket Survival Guide - OS</title>
</head>

<body> 
<div class="navheader">
<table summary="Navigation header" width="100%">
  <tbody>
    <tr>
      <th colspan="9" align="center">
	  
<A HREF="http://tin6150.github.io/psg/">Sys Admin Pocket Survival Guide - Vast Data</A>

      </th>
    </tr>
    <tr>
      <td align="left">  <a accesskey="h" href="psg.html">Home</a>	</td>
      <td align="center"><a accesskey="s" href="sol.html">Solaris</a>	</td>
      <td align="center"><a accesskey="p" href="hpux.html">HP-UX</a>	</td>
      <td align="center"><a accesskey="a" href="aix.html">AIX</a>	</td>
      <td align="center"><a accesskey="n" href="netapp.html">NetApp</a>	</td>
      <td align="center"><a accesskey="e" href="emcCelerra.html">EMC(NAS)</a>	</td>
      <td align="center"><a accesskey="E" href="emc.html">EMC(SAN)</a>	</td>
      <td align="center"><a accesskey="v" href="veritas.html">Veritas</a>	</td>
      <td align="right"> <a accesskey="l" href="ldap.html">LDAP</a>	</td>
    </tr>
  </tbody>
</table>
<hr></div>

<div class="chapter" lang="en">
<div class="titlepage">
</div>
</div>


<H1>Vast Data</H1>

the flash based file server

<BR><HR><BR>

<H1>HW</H1>


<PRE>

c-node: controller node.  4 of them, in a quad node 2U chassis
	rebooting c-node does not affect client, as client doesn't mount these directly (?)
	c-nodes are load balanced.
d-node: 2 node, each node has 4 bluefield nic, so show up as 8 d-box 
	these need 100G connection to NFS data network
	each d-node has a VIP, the client mount the VIP, which can migrate to other d-node when node is down.

switch.  1U 2 "half" switch.  Mellanox, 100G as "ethernet" mode switch internal use by Vast


there are no batteries in any of these (other than coin Li non rechargeable battery).
lead-acid are the batteries that can't tolerate higher temperature...


</PRE>

<H2>cmd</H2>

<PRE>

ssh  vast-server-management-ip  -l vastdata
ssh  192.8.21.50                -l vastdata




</PRE>

<PRE>
# jgi cycling to clear open inode due to bug from JW

-ssh into vastdata@mgmt00.vast0 
-run `vcli`
-run `cnode list`
	-make note of which owns the VMS, in the following instructions do that one last
-for each ID listed, do
	-`cnode deactivate --id {id from above}`
	- (wait for it to deactivate)
	-`cnode activate --id {id}`
-the last one might boot you out of the vms, just ping mgmt00.vast0 until it comes back and log back in, start vcli again, etc

</PRE>

<!-- gui method, see gmail socks5 proxy with video file link in gdrive -->

<H2>New System Setup</H2>

<PRE>

ssh  vast-server-management-ip  -l vastdata

vcli	# start the vast cli, to run vast config commands
admin/...

cnode list		# long table listing all controller nodes
vippool create ?	# -h or --help
vippool create --start-ip 10.8.16.50 --end-ip 10.8.16.57 --name primary_data --subnet-cidr 24 --gw-ip 10.8.16.1


vippool modify ?
vippool modify --id 2 --vlan 16		# set vippool to use vlan tagging (use 0 for no tag)


</PRE>


<H2>NFS exports config</H2>

<PRE>
# default root of the nvme pool is exported with default policy.

view list		# show nfs export, policy

viewpolicy list 	# export policy (root squash, etc)

viewpolicy show --id 1	
viewpolicy modify --id 1 --nfs-no-squash  10.8.21.22	# nfs management node IP
# [ we may have a routing problem that vast cluster and wwulf cluster use same subnet IP range and may have route problem ]

</PRE>

<H2>Change passwords</H2>

3 users passwords to change for the vast system 
<BR>
(these can ssh into the system)

<PRE>
vcli
cluster set-password --vastdata {ENTER} # prompt for new password
cluster set-password --root
cluster set-password --ipmi

</PRE>


4 "managers" passwords to change. 
<BR>
Can change all these, system will work, just make sure save them in a safe place
<BR>
(but can just run the cmd, so long as can run vcli as "admin" ?)

<PRE>
vcli
manager list
manager modify --id 2 --prompt-password   # this will prompt for password so that it no longer show as clear text

# --id 2 # admin - used for login thru the web gui or vcli
# --id 4 # S3_keys_manager, used internally by VAST
# --id 1 # root
# --id 3 # support

</PRE>

<H2>
Vast internal cluster
</H2>

<PRE>
clush -g cnodes /usr/sbin/ip a | grep 10.8.16	# check see if NFS traffic network link is up (vippool use this)

# clush is like pdsh
# enp59 is the 100G interface 

ssh 172.16.128.1	# cnode internal ip
ethtool enp59s0f0	# check link connection

clush -a   uptime
clush -a   hostname


find-vms	# list all vms (run as docker containers)
find-leader	# see which node is the "master"

</PRE>

<H2>internals</H2>

not sure how much customer (sys admins) need to know about these

<PRE>




docker ps	# there are 3 containers running...
docker ps | grep vms 	# vm that runs on cnodes.


There is a cluster name of sort on display on the GUI and in email notifications.
But probably nothing in the cli login.  
(The primary IP is a virtual address assigned to 1 cnode at run time)

So, to have some sort of idea of cluster name when logged into cli, 
changed each cnode hostname so that "hostname" would return something meaningful.

On each cnode, do:

sudo vi /etc/vast-configure_network.py-params.ini
sudo /usr/bin/configure_network.py --load-params-from-file


</PRE>

<h1>Power Off</h1>

<LI> Note down where vms is last run, power on would need to start on same node

<PRE>

ssh vastdata@10.8.21.50 
find-vms   # make a note of this for power on!
clush -g cnodes docker ps |grep vms
# vms is the host of the cluster VIP and vcli works here (also host the web gui)


vcli	# login as admin
cluseter deactivate

clush -g cnodes 'touch /vast/data/DONT_RUN_LEADER'
clush -g cnodes 'ls -l /vast/data/DONT_RUN_LEADER'   # verify all flag files created correctly

vtool suicide
find-leader	# should not find


clush -a hostname

clush -g cnodes 'sudo /usr/sbin/shutdown -h +2' ; clush -g dnodes 'sudo /usr/sbin/shutdown -h +5'



</PRE>


<h1>Power On</h1>

see cluster_power_on.md
greta gitlab 

clush -g cnodes "ls -l /vast/data/vms-bringup.log"

<BR><HR><BR>
Testing area <BR>

<h1>This is H1 text</h1>
<h2>This is H2 text
</h2>
<h3>This is H3 text
</h3>
<h4>This is H4 text</h4>
<h5>This is H5 text</h5>
<h6>This is H6 text</h6>

<EM>This is EM text</EM><BR>
<STRONG>This is STRONG text</STRONG><BR>
<BR>

<font face=monospace>
someple text here
	line with tab indent	more tab		yet more tab.
</font>
<BR>
<TT>
Typewriter monospaced fonts in here.
This is another line.
</TT>


<BR>
<BR>
<BR>


<BR><HR>
<div align="CENTER">
  [Doc URL: <A HREF="http://tin6150.github.io/psg/">
http://tin6150.github.io/psg/
</A>] <BR>
Last Updated: 2008-03-22
  <!--[Doc URL: <A HREF="http://tin6150.github.io/psg/">http://tin6150.github.io/psg/</A>] <BR>-->
<BR>
(cc) Tin Ho. See 
<A HREF=psg.html#cc>main page</A>
 for copyright info. <BR>
<HR>
<A HREF="http://www.taos.com"><IMG SRC="banner/taos_banner1.gif" width="728" height="98"></A>
</div>
<div class="sig"><BR>
  hoti1<BR>
  sn5050<BR>
  psg101 sn50 tin6150</div>


</body>

<!-- Google analytics new tracking code ga.js.   Will actually need to add this code to every page for full tracking!    (still the case in 2011?) Using my gmail login 2011.0617 updated with code for http://tin6150.github.io/psg/psg.html -->    <script type="text/javascript">    var _gaq = _gaq || [];   _gaq.push(['_setAccount', 'UA-4515095-4']);   _gaq.push(['_trackPageview']);    (function() {     var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;     ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';     var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);   })();  </script>


</html>
